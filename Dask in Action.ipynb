{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this BE we'll look at deploying Dask in two different ways, on an HPC cluster and on GCP using Kubernetes. You'll then use a Dask cluster to run a hyper-parameter search, which will be the basis for your grade. This notebook is just for presentation - you will write and hand in your own notebooks depending on which Dask installation you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "+ [Dask on Rainman](#rainman)\n",
    "+ [Dask on GCP with Kubernetes](#kubernetes)\n",
    "+ [Testing Dask](#testing)\n",
    "+ [HyperParameter Optimization](#hyperparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"rainman\">Dask on Rainman</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way we'll look at Dask is on Rainman, the High-Performance Computing cluster for students at ISAE. It is available at `rainman.isae.fr`, but only from internal networks. The architecture looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rainman](img/rainman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://icampus.isae-supaero.fr/rainman-architecture-materielle-du-supercalculateur-etudiant?lang=fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to rainman, do:\n",
    "```bash\n",
    "local$ ssh USER@rainmain\n",
    "```\n",
    "with your student username. You must be on the internal network.\n",
    "\n",
    "`dask` is already installed on rainman, so to use it, do:\n",
    "```bash\n",
    "rainman$ module load python/3.7\n",
    "rainman$ source activate dask\n",
    "rainman$ jupyter-notebook --no-browser --port=PORT --ip=IP\n",
    "```\n",
    "\n",
    "For `PORT`, pick a number between 8000 and 9999; if it doesn't work try a different one. For `IP`, if you are connected to `rainman1` use `10.162.9.3` and for `rainman2` use `10.162.9.4`. You should now be able to access the jupyter server from your machine at http://IP:PORT/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the jupyter server is working, we can use the Dask [SLURMCluster](https://jobqueue.dask.org/en/latest/generated/dask_jobqueue.SLURMCluster.html#dask_jobqueue.SLURMCluster) class to launch batch jobs, creating workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(cores=16,\n",
    "                       memory=\"60GB\",\n",
    "                       walltime=\"01:00:00\",\n",
    "                       dashboard_address=\":8787\", # different from before, change it if there's a warning\n",
    "                       job_extra=[\"--reservation=root_2\"], # only between 9h and 19h on 11/03/2020\n",
    "                       queue=\"p16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch the workers, run `scale`. Please don't run more than 2 jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the cluster, we can see the Dask dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you successfully run the client and can see the dashboard, you can move on to [Testing Dask](#testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"kubernetes\">Dask on GCP with Kubernetes</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second way we can use Dask is through Kubernetes on GCP. Kubernetes is a container orchestration system which can be used to deploy many different applications.\n",
    "![img/kubernetes.png]\n",
    "\n",
    "Follow the instructions [here](https://zero-to-jupyterhub.readthedocs.io/en/latest/google/step-zero-gcp.html) to set up Kubernetes. Once you've got helm verified (end of [this page](https://zero-to-jupyterhub.readthedocs.io/en/latest/setup-jupyterhub/setup-helm.html)), you can move on to the [Dask installation instructions](https://docs.dask.org/en/latest/setup/kubernetes-helm.html). You will need to create a `config.yaml` file and upload it to your GCP shell in order to install dask-ml on all your workers (section \"Configure Environment\").\n",
    "\n",
    "Once the Kubernetes cluster is running, you should have access to a JupyterHub notebook. To use Dask, you just need to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, config\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"testing\">Testing Dask</a>\n",
    "\n",
    "To test that our Dask installations are working, we'll look at a simple KMeans example. Copy the following code to your Dask notebook, whether on rainman or GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_ml.datasets\n",
    "import dask_ml.cluster\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale up: increase n_samples or n_features\n",
    "X, y = dask_ml.datasets.make_blobs(n_samples=1000000,\n",
    "                                   chunks=100000,\n",
    "                                   random_state=0,\n",
    "                                   centers=3)\n",
    "X = X.persist()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = dask_ml.cluster.KMeans(n_clusters=3, init_max_iter=2, oversampling_factor=10)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're done with a session, make sure to close your client. If you used rainman, also close the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close() # only for rainman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"hyperparam\">Hyperparameter Optimization</a>\n",
    "\n",
    "A task that works well in parallel computing is hyperparameter optimization. Machine learning models have a number of different parameters, and parameter combinations can be tested in parallel independently.\n",
    "\n",
    "For this assignment, you will perform hyperparameter optimization on the [Titanic](https://www.kaggle.com/c/titanic/data) dataset. You should follow the extensive hyperparameter optimization example in the [Dask documentation](https://examples.dask.org/machine-learning/hyperparam-opt.html). You will need to create a notebook in your Dask installation, and once you have it working, save the notebook and submit it in the LMS. \n",
    "\n",
    "To get started, download the Titanic data. You have to first [agree to the competition](https://www.kaggle.com/c/titanic/rules) and will need to [set your Kaggle API key](https://www.kaggle.com/docs/api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip titanic.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be graded on the following criteria:\n",
    "\n",
    "+ Reproducibility - does your notebook work when I run it? Make sure to specify which method (rainman or GCP)\n",
    "+ Completeness\n",
    "+ Prediction performance\n",
    "+ Compute performance - how fast is it?\n",
    "\n",
    "Bonus points for:\n",
    "+ Including Kaggle submission in your notebook\n",
    "+ Explanation (text and code comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
